# ----------------------------------------------------
# Project
# ----------------------------------------------------
project_name: llm-lora-finetune
seed: 42

# ----------------------------------------------------
# Model Configuration
# ----------------------------------------------------
model:
  model_name: microsoft/phi-3-mini-4k-instruct
  load_in_4bit: false
  dtype: auto

# ----------------------------------------------------
# Dataset & Preprocessing
# ----------------------------------------------------
data:
  dataset_path: data/raw/mydataset.jsonl

  processed_dir: data/processed
  train_path: data/processed/train
  val_path: data/processed/val
  test_path: data/processed/test

  max_length: 1024
  train_split: 0.8
  val_split: 0.1

# ----------------------------------------------------
# LoRA Configuration
# ----------------------------------------------------
lora:
  r: 8
  alpha: 16
  dropout: 0.05
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj

# ----------------------------------------------------
# Training Configuration
# ----------------------------------------------------
training:
  output_dir: outputs/phi-3

  num_train_epochs: 2
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 8

  learning_rate: 1.0e-4
  lr_scheduler_type: cosine
  warmup_ratio: 0.1

  logging_steps: 10
  eval_steps: 100

  report_to: none
  fp16: true
  bf16: false

# ----------------------------------------------------
# Evaluation Configuration
# ----------------------------------------------------
evaluation:
  eval_data_dir: data/processed/test
  batch_size: 2
  lora_output_dir: outputs/phi-3
  fp16: true
