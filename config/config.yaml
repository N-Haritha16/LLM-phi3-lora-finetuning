
# Project
project_name: llm-lora-finetune
seed: 42

# Model
model:
  model_name: microsoft/phi-3-mini-4k-instruct
  load_in_4bit: true
  dtype: auto

# Dataset
data:
  dataset_path: data/raw/mydataset.jsonl
  processed_dir: data/processed
  train_path: data/processed/train
  val_path: data/processed/val
  test_path: data/processed/test
  max_length: 512              # was 1024
  train_split: 0.8
  val_split: 0.1



project:
  project_name: llm-lora-finetune
  seed: 42

model:
  model_name: microsoft/phi-3-mini-4k-instruct
  load_in_4bit: true        # QLoRA / bitsandbytes required
  dtype: auto

data:
  dataset_path: data/raw/mydataset.jsonl
  processed_dir: data_proc
  train_path: data_proc/train
  val_path: data_proc/val
  test_path: data_proc/test
  max_length: 1024
  train_split: 0.8
  val_split: 0.1


lora:
  r: 4                         # was 8
  alpha: 8                     # was 16
  dropout: 0.05
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj



# Training
training:
  output_dir: outputs/
  num_train_epochs: 1          # was 3
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  learning_rate: 0.0002        # keep as float

training:
  output_dir: outputs/phi-3
  num_train_epochs: 3        # you can set 1 if CPU-only and too slow
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  learning_rate: 0.0002

  logging_steps: 10
  save_strategy: "epoch"
  save_total_limit: 3
  evaluation_strategy: "epoch"
  report_to: "wandb"

  run_name: "phi3_lora_run_1"

# Evaluation

  run_name: "phi3_lora_run_qloRA"
  fp16: true                 # OK for GPU; if CPU-only and it errors, set false


evaluation:
  eval_data_dir: data/processed/test   # not actually used, but fine
  batch_size: 2
  lora_output_dir: outputs/phi-3
  fp16: true
